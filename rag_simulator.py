#!/usr/bin/env python3
"""
RAG ì‹œë®¬ë ˆì´í„° v2.1

KMS ì§€ì‹ê·¸ë˜í”„ë¥¼ í™œìš©í•œ RAG(Retrieval-Augmented Generation) ì‹œë®¬ë ˆì´ì…˜.
ì‹¤ì œ RAG ì‹œìŠ¤í…œ êµ¬í˜„ ì „, ê²€ìƒ‰/ì „íŒŒ ë¡œì§ì„ ê²€ì¦í•©ë‹ˆë‹¤.

ì‹œë‚˜ë¦¬ì˜¤:
1. ìˆ˜ìˆ˜ë£Œ ì§ˆë¬¸ â†’ ì‹œì±…, ê³„ì‚°ê¸°ì¤€, ì •ì‚°ìë£Œ ì „íŒŒ
2. ìƒí’ˆ ë¹„êµ ì§ˆë¬¸ â†’ ìƒí’ˆì„¤ëª…ì„œ, ë¹„êµí‘œ, ì•½ê´€ ì „íŒŒ
3. ì‹¬ì‚¬ ì§ˆë¬¸ â†’ ì‹¬ì‚¬ê°€ì´ë“œ, ì§ˆë³‘ë³„/ì§ì—…ë³„ ê¸°ì¤€ ì „íŒŒ
4. ê·œì œ ì§ˆë¬¸ â†’ ë²•ë¥ , ê·œì •, ì»´í”Œë¼ì´ì–¸ìŠ¤ê°€ì´ë“œ ì „íŒŒ
"""

import json
import re
from dataclasses import dataclass, field
from typing import List, Dict, Set, Optional
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° êµ¬ì¡°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class Document:
    id: str
    name: str
    carrier: str
    product: str
    doc_type: str
    tier: str
    process: str = ""
    audience: str = ""
    content: str = ""
    relations: Dict = field(default_factory=dict)

@dataclass
class SearchResult:
    doc_id: str
    score: float
    reason: str
    propagated_from: Optional[str] = None

@dataclass
class RAGResponse:
    query: str
    primary_docs: List[SearchResult]
    propagated_docs: List[SearchResult]
    context_summary: str
    retrieval_time_ms: float

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG ì‹œë®¬ë ˆì´í„°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RAGSimulator:
    def __init__(self, graph_path: str = "knowledge_graph_v2.1.json"):
        self.documents: Dict[str, Document] = {}
        self.taxonomy = {}
        self.edges = []
        self.load_graph(graph_path)
        
    def load_graph(self, path: str):
        """ì§€ì‹ê·¸ë˜í”„ ë¡œë“œ"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.taxonomy = data.get('taxonomy', {})
        
        # ë…¸ë“œ â†’ ë¬¸ì„œ
        for node in data.get('graph_data', {}).get('nodes', []):
            if 'Document' not in node.get('labels', []):
                continue
            
            props = node.get('properties', {})
            doc_type = next((l for l in node['labels'] if l.startswith('DOC-')), '')
            tier = next((l for l in node['labels'] if l in ['HOT', 'WARM', 'COLD']), 'COLD')
            
            self.documents[node['id']] = Document(
                id=node['id'],
                name=props.get('name', ''),
                carrier=props.get('carrier', ''),
                product=props.get('product', ''),
                doc_type=doc_type,
                tier=tier,
                process=props.get('process', ''),
                audience=props.get('audience', ''),
                content=props.get('content', ''),
                relations={'siblings': [], 'references': [], 'referenced_by': []}
            )
        
        # ì—£ì§€ â†’ ê´€ê³„
        for edge in data.get('graph_data', {}).get('edges', []):
            src = edge.get('source', '')
            tgt = edge.get('target', '')
            rel = edge.get('type', '')
            
            if src in self.documents and tgt in self.documents:
                if rel == 'SIBLINGS':
                    self.documents[src].relations['siblings'].append(tgt)
                elif rel == 'REFERENCES':
                    self.documents[src].relations['references'].append(tgt)
                    self.documents[tgt].relations['referenced_by'].append(src)
        
        print(f"âœ“ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
    
    def keyword_match(self, query: str) -> List[SearchResult]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ 1ì°¨ ê²€ìƒ‰"""
        results = []
        query_lower = query.lower()
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(query)
        
        for doc_id, doc in self.documents.items():
            score = 0.0
            reasons = []
            
            # ë¬¸ì„œëª… ë§¤ì¹­
            if any(kw in doc.name.lower() for kw in keywords):
                score += 0.5
                reasons.append("ë¬¸ì„œëª… ë§¤ì¹­")
            
            # ë¬¸ì„œìœ í˜• ë§¤ì¹­
            doc_type_name = self.taxonomy.get('doc_types', {}).get(doc.doc_type, {}).get('name', '')
            if any(kw in doc_type_name.lower() for kw in keywords):
                score += 0.4
                reasons.append("ë¬¸ì„œìœ í˜• ë§¤ì¹­")
            
            # í”„ë¡œì„¸ìŠ¤ ë§¤ì¹­
            process_name = self.taxonomy.get('processes', {}).get(doc.process, {}).get('name', '')
            if any(kw in process_name.lower() for kw in keywords):
                score += 0.3
                reasons.append("í”„ë¡œì„¸ìŠ¤ ë§¤ì¹­")
            
            # Tier ê°€ì¤‘ì¹˜
            tier_bonus = {'HOT': 0.2, 'WARM': 0.1, 'COLD': 0.0}
            score += tier_bonus.get(doc.tier, 0)
            
            if score > 0:
                results.append(SearchResult(
                    doc_id=doc_id,
                    score=min(score, 1.0),
                    reason=", ".join(reasons)
                ))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:10]  # ìƒìœ„ 10ê°œ
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ì˜', 'ë¥¼', 'ì„', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€'}
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', query.lower())
        return [w for w in words if w not in stopwords and len(w) > 1]
    
    def propagate(self, primary_docs: List[SearchResult], max_depth: int = 2) -> List[SearchResult]:
        """ê´€ê³„ ê¸°ë°˜ ì „íŒŒ"""
        propagated = []
        visited = {r.doc_id for r in primary_docs}
        
        def _propagate_from(doc_id: str, depth: int, from_id: str):
            if depth > max_depth:
                return
            
            doc = self.documents.get(doc_id)
            if not doc:
                return
            
            # í˜•ì œ ë¬¸ì„œ (ì „íŒŒ ì ìˆ˜ ê°ì†Œ)
            for sib_id in doc.relations.get('siblings', []):
                if sib_id not in visited:
                    visited.add(sib_id)
                    propagated.append(SearchResult(
                        doc_id=sib_id,
                        score=0.7 / depth,
                        reason=f"í˜•ì œê´€ê³„ (depth={depth})",
                        propagated_from=from_id
                    ))
                    _propagate_from(sib_id, depth + 1, sib_id)
            
            # ì°¸ì¡° ë¬¸ì„œ
            for ref_id in doc.relations.get('references', []):
                if ref_id not in visited:
                    visited.add(ref_id)
                    propagated.append(SearchResult(
                        doc_id=ref_id,
                        score=0.6 / depth,
                        reason=f"ì°¸ì¡°ê´€ê³„ (depth={depth})",
                        propagated_from=from_id
                    ))
                    _propagate_from(ref_id, depth + 1, ref_id)
            
            # ì—­ì°¸ì¡° (ì´ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ëŠ” ë¬¸ì„œ)
            for ref_by_id in doc.relations.get('referenced_by', []):
                if ref_by_id not in visited:
                    visited.add(ref_by_id)
                    propagated.append(SearchResult(
                        doc_id=ref_by_id,
                        score=0.5 / depth,
                        reason=f"ì—­ì°¸ì¡° (depth={depth})",
                        propagated_from=from_id
                    ))
        
        # 1ì°¨ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì „íŒŒ ì‹œì‘
        for result in primary_docs[:5]:  # ìƒìœ„ 5ê°œì—ì„œë§Œ ì „íŒŒ
            _propagate_from(result.doc_id, 1, result.doc_id)
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        propagated.sort(key=lambda x: x.score, reverse=True)
        return propagated[:15]  # ìƒìœ„ 15ê°œ
    
    def generate_context(self, primary: List[SearchResult], propagated: List[SearchResult]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        all_docs = primary + propagated
        if not all_docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        # ë¬¸ì„œ ìœ í˜•ë³„ ê·¸ë£¹í™”
        by_type = {}
        for r in all_docs[:10]:
            doc = self.documents.get(r.doc_id)
            if doc:
                doc_type_name = self.taxonomy.get('doc_types', {}).get(doc.doc_type, {}).get('name', doc.doc_type)
                if doc_type_name not in by_type:
                    by_type[doc_type_name] = []
                by_type[doc_type_name].append(doc.name)
        
        lines = ["[ê²€ìƒ‰ëœ ë¬¸ì„œ]"]
        for doc_type, names in by_type.items():
            lines.append(f"- {doc_type}: {', '.join(names[:3])}")
        
        return "\n".join(lines)
    
    def search(self, query: str) -> RAGResponse:
        """RAG ê²€ìƒ‰ ìˆ˜í–‰"""
        import time
        start = time.time()
        
        # 1ì°¨: í‚¤ì›Œë“œ ê²€ìƒ‰
        primary = self.keyword_match(query)
        
        # 2ì°¨: ê´€ê³„ ì „íŒŒ
        propagated = self.propagate(primary)
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = self.generate_context(primary, propagated)
        
        elapsed = (time.time() - start) * 1000
        
        return RAGResponse(
            query=query,
            primary_docs=primary,
            propagated_docs=propagated,
            context_summary=context,
            retrieval_time_ms=elapsed
        )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_scenarios():
    """RAG ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 70)
    print("KMS RAG Simulator v2.1")
    print("=" * 70)
    print()
    
    rag = RAGSimulator()
    
    scenarios = [
        {
            "name": "ì‹œë‚˜ë¦¬ì˜¤ 1: ìˆ˜ìˆ˜ë£Œ ì§ˆë¬¸",
            "query": "ì‚¼ì„±ìƒëª… ì¢…ì‹ ë³´í—˜ ìˆ˜ìˆ˜ë£Œê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "expected_types": ["DOC-COMMISSION", "DOC-INCENTIVE", "DOC-COMMISSION-CALC", "DOC-SETTLEMENT"]
        },
        {
            "name": "ì‹œë‚˜ë¦¬ì˜¤ 2: ìƒí’ˆ ë¹„êµ",
            "query": "ì–´ë¦°ì´ë³´í—˜ ìƒí’ˆ ë¹„êµí•´ì£¼ì„¸ìš”",
            "expected_types": ["DOC-COMPARISON", "DOC-GUIDE", "DOC-TERMS"]
        },
        {
            "name": "ì‹œë‚˜ë¦¬ì˜¤ 3: ì‹¬ì‚¬ ë¬¸ì˜",
            "query": "ë‹¹ë‡¨ í™˜ì ì‹¬ì‚¬ ê°€ëŠ¥í•œê°€ìš”?",
            "expected_types": ["DOC-UW-GUIDE", "DOC-UW-DISEASE", "DOC-UW-RULE"]
        },
        {
            "name": "ì‹œë‚˜ë¦¬ì˜¤ 4: ê·œì œ ë¬¸ì˜",
            "query": "1200%ë£° GA ì ìš© ì–¸ì œë¶€í„°ì¸ê°€ìš”?",
            "expected_types": ["DOC-LAW-INSURANCE", "DOC-REGULATION", "DOC-COMPLIANCE-GUIDE"]
        }
    ]
    
    results = []
    
    for scenario in scenarios:
        print(f"\n{'â”€' * 70}")
        print(f"ğŸ“‹ {scenario['name']}")
        print(f"â“ ì¿¼ë¦¬: {scenario['query']}")
        print(f"{'â”€' * 70}")
        
        response = rag.search(scenario['query'])
        
        # 1ì°¨ ê²€ìƒ‰ ê²°ê³¼
        print(f"\nğŸ” 1ì°¨ ê²€ìƒ‰ ê²°ê³¼ ({len(response.primary_docs)}ê±´):")
        found_types = set()
        for r in response.primary_docs[:5]:
            doc = rag.documents.get(r.doc_id)
            if doc:
                found_types.add(doc.doc_type)
                print(f"   â€¢ [{r.score:.2f}] {doc.name}")
                print(f"     â”” {r.reason}")
        
        # ì „íŒŒ ê²°ê³¼
        print(f"\nğŸ”— ì „íŒŒ ê²°ê³¼ ({len(response.propagated_docs)}ê±´):")
        for r in response.propagated_docs[:5]:
            doc = rag.documents.get(r.doc_id)
            if doc:
                found_types.add(doc.doc_type)
                from_doc = rag.documents.get(r.propagated_from)
                from_name = from_doc.name if from_doc else r.propagated_from
                print(f"   â€¢ [{r.score:.2f}] {doc.name}")
                print(f"     â”” {r.reason} â† {from_name}")
        
        # ì»¨í…ìŠ¤íŠ¸
        print(f"\nğŸ“„ ì»¨í…ìŠ¤íŠ¸:")
        for line in response.context_summary.split('\n'):
            print(f"   {line}")
        
        # ê²€ì¦
        expected_set = set(scenario['expected_types'])
        matched = found_types & expected_set
        match_rate = len(matched) / len(expected_set) * 100 if expected_set else 0
        
        print(f"\nâœ… ê²€ì¦:")
        print(f"   ê¸°ëŒ€ ìœ í˜•: {', '.join(expected_set)}")
        print(f"   ë°œê²¬ ìœ í˜•: {', '.join(found_types)}")
        print(f"   ë§¤ì¹­ë¥ : {match_rate:.1f}%")
        print(f"   ê²€ìƒ‰ ì‹œê°„: {response.retrieval_time_ms:.2f}ms")
        
        results.append({
            "scenario": scenario['name'],
            "match_rate": match_rate,
            "primary_count": len(response.primary_docs),
            "propagated_count": len(response.propagated_docs),
            "time_ms": response.retrieval_time_ms
        })
    
    # ìš”ì•½
    print(f"\n{'â•' * 70}")
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½")
    print(f"{'â•' * 70}")
    
    total_match = sum(r['match_rate'] for r in results) / len(results)
    total_time = sum(r['time_ms'] for r in results)
    
    for r in results:
        status = "âœ…" if r['match_rate'] >= 50 else "âš ï¸"
        print(f"{status} {r['scenario']}: ë§¤ì¹­ë¥  {r['match_rate']:.1f}%, "
              f"1ì°¨ {r['primary_count']}ê±´, ì „íŒŒ {r['propagated_count']}ê±´")
    
    print(f"\ní‰ê·  ë§¤ì¹­ë¥ : {total_match:.1f}%")
    print(f"ì´ ê²€ìƒ‰ ì‹œê°„: {total_time:.2f}ms")
    
    return results


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    try:
        results = run_scenarios()
        
        # JSON ê²°ê³¼ ì €ì¥
        output = {
            "timestamp": datetime.now().isoformat(),
            "version": "2.1",
            "results": results,
            "summary": {
                "total_scenarios": len(results),
                "avg_match_rate": sum(r['match_rate'] for r in results) / len(results),
                "avg_time_ms": sum(r['time_ms'] for r in results) / len(results)
            }
        }
        
        with open('docs/rag_simulation_results.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ ê²°ê³¼ ì €ì¥: docs/rag_simulation_results.json")
        
    except FileNotFoundError:
        print("âŒ knowledge_graph_v2.1.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € simulator_v2_extended.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
